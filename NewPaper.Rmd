---
title: "CO2 Emissions in Modern Vehicles"
author: "Hari Prasannaa Thangavel Ravi, Jin Hyuk Son, Pranjal S. Wakpaijan"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
---

```{r init, include=FALSE}
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course. 
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
library(ezids)
 
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "markup", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

### **Abstract**

With growing concerns about environmental sustainability and the impact of greenhouse gases, this project investigates factors influencing carbon dioxide (CO2) emissions from vehicles, focusing on models released between 2020 and 2024. Using a dataset comprising 40,000 vehicle records with 85 detailed attributes, sourced from the U.S. Department of Energy (DOE), the relationships between vehicle characteristics and the amount of CO2 emitted from a vehicle's tailpipe are analyzed. Additionally, three key SMART questions were addressed to provide deeper insights into the data. The study addresses three objectives: constructing a linear regression model to quantify the impact of vehicle features on CO2 emissions, clustering vehicles based on their attributes to identify distinct groups and their defining characteristics, and implementing a classification tree to predict vehicle sustainability based on emissions. The goal is to offer insights into the variables that critically impact CO2 emissions in modern vehicles and provide perspectives that could support environmental policies.

```{r libraries}
#Load necessary libraries
library(devtools)
library(dplyr)
library(ezids)
library(caret)
library(leaps)
library(car)
library(tidyverse)
```

### **Introduction**

Transportation accounts for a significant share of global greenhouse gas emissions, with vehicles contributing to a substantial share of carbon dioxide (CO2) emissions throughout the world. According to the U.S. Environmental Protection Agency (EPA), the vehicle fuel economy has reached a record high, and the greenhouse gas emissions dropped to record low in 2023. Advancement in automotive technology and stricter regulations led to stronger motivations for improvement in fuel efficiency and reduction in emissions, resulting in average miles per gallon (MPG) increase from 13.1 to 27.1 since 1975 (EPA, 2021).

This project explores the relationship between vehicle characteristics and CO₂ emissions using fuel economy data from the U.S. Department of Energy. The dataset includes vehicle testing results conducted by the EPA or vehicle manufacturers under EPA oversight (U.S. Department of Energy, 2024). For this study, we focus exclusively on vehicles from 2020 to 2024, analyzing attributes such as engine displacement, drivetrain type, and fuel type, with tailpipe CO₂ emissions (in grams per mile) serving as the dependent variable. Electric vehicles (EVs) are excluded due to their fundamentally different emissions profile.

Through this analysis, we aim to provide insight on features that contribute to CO2 emissions and identify characteristics of sustainable vehicles. First, linear regression is applied to understand the influence of vehicle features on emissions. Next, clustering techniques reveal groups of vehicles with shared characteristics, enabling insights into emission patterns across different vehicle types. Finally, a classification tree is developed to predict a vehicle’s sustainability. Through our findings, we hope to provide actionable insights for policymakers and automotive manufacturers in their efforts to mitigate environmental impacts.

```{r load_dataset}
#load the dataset
df <- read.csv("C:\\Users\\HP\\OneDrive\\Documents\\DATS6101-Team4-Project2\\Fuel Economy Data.csv")
```

```{r data_summary}
summary(df)
```

```{r remove_ev}

#defining the unwanted columns (columns realted to fuel type 2 and ev)
unwantedlist <- c(
  "barrelsA08",
  "cityA08",
  "cityA08U",
  "co2A",
  "co2TailpipeAGpm",
  "combA08",
  "combA08U",
  "fuelCostA08",
  "fuelType2",
  "highwayA08",
  "highwayA08U",
  "rangeA",
  "rangeCityA",
  "rangeHwyA",
  "UCityA",
  "UHighwayA",
  "charge120",
  "charge240",
  "sCharger",
  "tCharger",
  "c240Dscr",
  "charge240b",
  "c240bDscr",
  "phevHwy",
  "phevCity",
  "phevComb",
  "modifiedOn",
  "cityCD",
  "combinedCD",
  "highwayCD",
  "phevBlended",
  "cityE",
  "combE",
  "combinedUF",
  "evMotor",
  "highwayE",
  "highwayUF",
  "cityUF",
  "rangeCity",
  "rangeHwy",
  "range",
  "hlv",
  "hpv",
  "pv2",
  "pv4",
  "lv2",
  "lv4",
  "comb08U",
  "city08U",
  "highway08U",
  "engId"
)
cleaned_df <- df %>% select(-all_of(unwantedlist))

#considering only from year 2013
cleaned_df <- cleaned_df %>%
  filter(year %in% (2020:2024))

#removing ev vehicles
cleaned_df <- cleaned_df[!is.na(cleaned_df$cylinders) & cleaned_df$cylinders != "Electricity",]
```

```{r check_missing}
#check the number of missing values for each column
missing_values <- colSums(is.na(cleaned_df))
missing_values
```

```{r zero_count}
#check the number of value 0 for each column
zero_count_per_column <- cleaned_df %>%
  summarise(across(everything(), ~sum(. == 0, na.rm = TRUE)))

zero_count_per_column
```

```{r remove_columns}
#remove columns that are for car identification
cleaned_df <- cleaned_df %>%select(-c(createdOn,mfrCode,baseModel,model,make,id,mpgData,year))
#remove redundant columns and ghg score and fescore, which would be another dependent variable
cleaned_df <- cleaned_df %>%select(-c(co2TailpipeGpm, fuelType, feScore, ghgScore, ghgScoreA,fuelType1))
#remove empty column
cleaned_df <- cleaned_df %>%select(-trans_dscr,atvType)
```

```{r encode_cat_vars}
#Identify categorical columns and their unique value counts
categorical_columns <- sapply(cleaned_df, function(col) is.character(col) || is.factor(col))
unique_values <- sapply(cleaned_df[, categorical_columns], function(col) length(unique(col)))

#Separate columns for one-hot and label encoding
one_hot_columns <- names(unique_values[unique_values <= 10]) # Threshold for one-hot encoding
label_columns <- names(unique_values[unique_values > 10])

#Applying One-Hot Encoding
if (length(one_hot_columns) > 0) {
  print(one_hot_columns)
  one_hot_encoded <- dummyVars(~ ., data = cleaned_df[one_hot_columns], fullRank = TRUE)
  one_hot_df <- data.frame(predict(one_hot_encoded, newdata = cleaned_df))
}

#Applying Label Encoding
for (column in label_columns) {
  cleaned_df[[column]] <- as.numeric(as.factor(cleaned_df[[column]]))
}

#Combining one-hot encoded and remaining data
final_data <- bind_cols(cleaned_df %>% select(-one_hot_columns), one_hot_df)

#View the first few rows of the encoded dataset
print(head(final_data))
```

```{r remove_add_columns}
#removing categorical variables with two many categories - avoid overfitting
linear_df <- final_data %>%select(-c(trany, eng_dscr, VClass))
#remove any mpg because co2 is directly related to mpg (which is tailpipe CO2 in grams/mile)
#also remove barrels08 as it is basically proportional to co2
#linear_df <- linear_df %>%select(-c(comb08, UCity, UHighway, highway08, city08, barrels08))
linear_df <- linear_df %>%select(-c(comb08,UCity,UHighway,highway08, city08, barrels08))
```

```{r regsubset}
library(leaps)
regfit_full <- regsubsets(co2 ~ ., data = linear_df, nvmax = 35)
reg_summary <- summary(regfit_full)
```

```{r best_size}
# Find the model size using adjusted r squared
best_model_size <- which.max(reg_summary$adjr2)
best_model_size
```

```{r best_cols}
#Column names of the best model size
selected_vars <- names(which(reg_summary$which[best_model_size, ])) %>%
  setdiff("(Intercept)")  # Remove intercept

#filter for the column names above
selected_data <- final_data %>%
  select(all_of(selected_vars)) %>%
  mutate(co2 = final_data$co2) #add our dependent variable
```

```{r check_vif}
#We are going to run VIF to check multicollinearity

vif_model <- lm(co2 ~ ., data = selected_data)

vif_values <- vif(vif_model)

vif_df <- as.data.frame(vif_values) %>%
  rownames_to_column(var = "Variable")
print(vif_df)
```

```{r remove_high_vif}
#remove the highest VIF
lm_data <- selected_data %>%
  select(-youSaveSpend)
#lm_data <- lm_data %>%select(-cylinders)
lm_data <- lm_data %>%
  select(-fuelCost08)

#run vif again
vif_model <- lm(co2 ~ ., data = lm_data)
vif_values <- vif(vif_model)
vif_df <- as.data.frame(vif_values) %>%
  rownames_to_column(var = "Variable")
print(vif_df)
```
### **Research Question 1**

How do key vehicle attributes, including engine size, drivetrain type, and hybrid status, impact CO₂ emissions in vehicles from 2020 to 2024, and how accurately can the model predict CO₂ emissions compared to actual observed values?

#### **Method**
we employed a linear regression model to quantify the relationships between CO₂ emissions (the dependent variable) and key vehicle features.The model was trained on data from 2020 to 2024 and validated on a separate test set to evaluate its predictive performance.

```{r}
lm_model <- lm(co2 ~ ., data = lm_data)
summary(lm_model)
```
```{r stats}

colSums(is.na(lm_data))
```

```{r corr}
ggplot(lm_data, aes(x = displ, y = co2)) + 
  geom_point(alpha = 0.5, color = "blue") +
  geom_smooth(method = "lm", color = "red") +
  labs(title = "CO₂ Emissions vs. Engine Displacement", x = "Engine Displacement", y = "C02 Emissions")

```
 A red line, representing the linear regression line, is overlaid on the scatter plot. This line summarizes the overall trend in the data, showing that there is a positive linear relationship between engine displacement and CO2 emissions.As engine displacement increases, CO2 emissions tend to rise as well. This relationship makes sense as larger engines generally consume more fuel, leading to higher emissions.The data points exhibit some clustering and spread. While there is a general upward trend, there is also variability in CO2 emissions for vehicles with similar engine displacements. This variation could be attributed to factors like engine efficiency, vehicle weight, and driving conditions.


```{r Residual Analysis}
# Fit the model
lm_model <- lm(co2 ~ ., data = lm_data)

# Residuals vs Predicted
predicted <- lm_model$fitted.values
residuals <- lm_model$residuals

ggplot(data.frame(Predicted = predicted, Residuals = residuals), aes(x = Predicted, y = Residuals)) +
  geom_point(alpha = 0.6) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs Predicted", x = "Predicted Values", y = "Residuals") +
  theme_minimal()

# Q-Q plot for normality of residuals
qqnorm(residuals)
qqline(residuals, col = "red")
```
Residuals vs Predicted

The points appear randomly scattered around the horizontal line at zero. This is a good sign, suggesting that the model is fitting the data well, and there is no clear relationship between the residuals and the predicted values.
There are a few outliers, especially on the higher end of the predicted values. Further investigation of these outliers is recommended to understand their impact on the model's overall fit.



```{r model validation}

set.seed(123)
train_index <- sample(seq_len(nrow(lm_data)), size = 0.8 * nrow(lm_data))
train_data <- lm_data[train_index, ]
test_data <- lm_data[-train_index, ]

lm_train_model <- lm(co2 ~ ., data = train_data)

test_predictions <- predict(lm_train_model, newdata = test_data)
test_residuals <- test_data$co2 - test_predictions


rmse <- sqrt(mean(test_residuals^2))
r_squared <- 1 - (sum(test_residuals^2) / sum((test_data$co2 - mean(test_data$co2))^2))

print(paste("Test RMSE:", round(rmse, 2)))
print(paste("Test R-squared:", round(r_squared, 2)))

```

Model Validation: We split the data into training and test sets using an 80/20 split.
A linear model was fitted using the lm() function in R, including all relevant predictors.
Model evaluation was done using metrics such as RMSE (Root Mean Squared Error) and R-squared. The results indicated a Test RMSE of 47.95 and a Test R-squared of 0.81, suggesting a strong model fit with good predictive power.



```{r viz}
ggplot(data.frame(Actual = test_data$co2, Predicted = test_predictions), aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.6, color = "blue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Predicted vs. Actual CO2 Emissions", x = "Actual CO2", y = "Predicted CO2") +
  theme_minimal()

```
#### **Analysis**
The linear regression model provided insights into how different vehicle attributes affect CO₂ emissions:
The model explains 81% of the variability in CO₂ emissions on the test dataset. This is a strong indication that the model captures most of the patterns and relationships within the data. The remaining 19% of the variance is unexplained by the model, suggesting that there may be other factors influencing CO₂ emissions not captured by the current set of predictors.
The graph illustrates the performance of a model predicting CO2 emissions. The model generally predicts CO2 emissions well as the data points are clustered around the red dashed line. However, there are several data points that are far from the ideal line, indicating that the model underestimates or overestimates CO2 emissions in some cases. 



### **Research Question 2**

How can clustering vehicle attributes reveal distinct groups of vehicles, and which group exhibits the lowest CO2 emissions, making them the most sustainable, among models from 2020 to 2024?

#### **Method**

In this analysis, we have clustered vehicles based on important features identified in earlier finding. To do so, we have used K-means clustering method. K-means clustering is an unsupervised machine learning method that assigns data points to one of the clusters based on their distance from the center of each cluster. The algorithm initially assigns centroid randomly. It then recalculates the centroids based on the mean of the assigned data points and iterate this process until the centroids no longer change significantly.

To choose the number of clusters, we have applied the Elbow Method. The method is described as follows: - *Calculate the Within-Cluster-Sum of Squared Errors (WSS) for different values of k, and choose the k for which WSS becomes first starts to diminish. In the plot of WSS-versus-k, this is visible as an elbow.* (Mahendru, 2019)

```{r elbow_method}
library(factoextra)
cluster_df <- lm_data

# Elbow method
fviz_nbclust(cluster_df, kmeans, method = "wss") +
    geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")
```

The point at which the WSS curve began to flatten suggested that 4 clusters would be the most appropriate number, as adding more clusters beyond this point would not yield a significant improvement.

#### **Analysis**

We applied K-means clustering with an optimal number of 4 clusters and examined how key vehicle features relate to CO₂ emissions across these clusters. To aid in visualizing the clustering results, we employed Principal Component Analysis (PCA) to reduce the dimensionality of the data, allowing us to plot the clusters in a two-dimensional space. The resulting scatter plot clearly showed the separation of vehicles into four distinct clusters.

```{r}
# Compute k-means with k = 4
set.seed(123)
#scaling
scaled_df <- scale(cluster_df)
km_res <- kmeans(scaled_df, 4, nstart = 25)
# Add cluster assignments to the original data for visualization
cluster_df$Cluster <- as.factor(km_res$cluster)

# PCA for dimensionality reduction
pca_result <- prcomp(scaled_df)
pca_df <- data.frame(pca_result$x)

# Plotting with ggplot2
ggplot(pca_df, aes(x = PC1, y = PC2, color = cluster_df$Cluster)) +
  geom_point() +
  stat_ellipse(level = 0.95) +  # Add ellipses around clusters (95% confidence intervals)
  labs(title = "K-means Clustering",
       x = "Principal Component 1", y = "Principal Component 2", color = "Clusters") +
  theme_minimal()
```

To better understand the characteristics that define each cluster, we examined the distribution of engine displacement across the four groups. The boxplot revealed that Cluster 2 consistently exhibited higher engine displacement compared to the other clusters. To statistically validate this observation, we conducted an ANOVA test, which returned a low p-value, indicating a significant difference in engine displacement between the clusters.

```{r eng_dspl_cluster}
# Create boxplots for engine displacement
ggplot(cluster_df, aes(x = Cluster, y = displ, fill = Cluster)) +
  geom_boxplot() +
  labs(title = "Engine Displacement by Cluster", x = "Cluster", y = "Engine Displacement")
# ANOVA for 'displ' across clusters
anova_displ <- aov(displ ~ Cluster, data = cluster_df)
summary(anova_displ)
```

Similarly, we investigated how CO2 emissions were distributed among the clusters. A similar pattern emerged, with Cluster 2 again showing the highest CO2 emissions, followed by Cluster 1, Cluster 3, and Cluster 4 in descending order. To further confirm this finding, we performed another ANOVA test, which also yielded a significant result, suggesting that the differences in CO2 emissions across clusters were meaningful.

```{r eng_dspl}
# Create boxplots for co2 by cluster
ggplot(cluster_df, aes(x = Cluster, y = co2, fill = Cluster)) +
  geom_boxplot() +
  labs(title = "CO2 Emissions by Cluster", x = "Cluster", y = "CO2 Emissions")
# ANOVA for 'displ' across clusters
anova_co2 <- aov(displ ~ co2, data = cluster_df)
summary(anova_co2)
```

We can observe a similar pattern to distribution of engine displacement. Cluster 2 had the highest CO2 with Cluster 1, Cluster 3, and Cluster 4 following in order. ANOVA test again shows that there is a significant difference between average CO2 emissions across the clusters.

Based on the mean values for each feature across the 4 clusters, we can describe the defining characteristics of each group as follows:

```{r}
# Calculate the mean of each numeric column grouped by cluster
cluster_means <- cluster_df %>%
  group_by(Cluster) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))


# View the mean of all columns by cluster
xkabledplyhead(cluster_means, 4)
```

-   Cluster 1

    -   Moderate to high numbers for cylinder (6.27) and engine displacement (3.77).

    -   Majority of the vehicles are rear-wheel drive.

-   Cluster 2

    -   High numbers for cylinder (9.48) and engine displacement (5.35).

    -   Either all-wheel drive or rear-wheel drive vehicles.

    -   Mostly gas guzzlers, indicating very low fuel efficiency. Likely to be larger vehicles such as trucks.

-   Cluster 3

    -   Moderate number for cylinders (5.15) and low number for engine displacement (2.70).

    -   Mostly all-wheel drive vehicles and characterized by start-stop systems.

-   Cluster 4

    -   Low numbers for cylinders (4.18) and engine displacement (2.15).

    -   Mostly front-wheel drive vehicles, likely to be compact cars.

By examining these clusters, we can draw several conclusions about sustainability and emissions. Cluster 2, with its larger engines and higher emissions, represents the least sustainable group, while Clusters 3 and 4, with their smaller engines and lower emissions, reflect more fuel-efficient, environmentally friendly vehicles. Cluster 1 falls somewhere in between, with moderate emissions and a mix of vehicle types. This analysis helps identify which vehicle features contribute most to CO₂ emissions and which types of vehicles are more sustainable.

### **References**

EPA Press Office. (2024, November 25). EPA Report Shows US Fuel Economy Hits Record High and CO2 Emissions Reach a Record Low. EPA. <https://www.epa.gov/newsreleases/epa-report-shows-us-fuel-economy-hits-record-high-and-co2-emissions-reach-record-low#>:\~:text=For%20MY%202023%2C%20new%20vehicle,of%20319%20grams%20per%20mile.

Mahendru, K. (2019, June 17). *How to determine the optimal K for K-means?*. Medium. <https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb>

U.S. Department of Energy. (n.d.). *Fueleconomy.gov web services*. www.fueleconomy.gov. <https://www.fueleconomy.gov/feg/ws/index.shtml#vehicle>
