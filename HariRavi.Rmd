---
title: "Project2"
date: "`r Sys.Date()`"
output:
  html_document:
  code_folding: hide
number_sections: false
toc: yes
toc_depth: 3
toc_float: yes
pdf_document:
  toc: yes
toc_depth: '3'
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
#Load necessary libraries
library(devtools)
library(dplyr)
library(ezids) 
library(caret)
library(rpart)
library(ggplot2)
library(lattice)
```
```{r pi}
#load the dataset
df <- read.csv('/Users/hariprasannaa/Downloads/vehicles.csv')

#display top rows of the dataset
xkabledplyhead(df, title = "Project2")

#summary of the dataset
summary(df)
```

```{r}
#display the shape of dataset
dim(df)
```

```{r col}
#display the column names of dataset
colnames(df)
```

```{r modify}

#defining the unwanted columns (columns realted to fuel type 2 and ev)
unwantedlist <- c(
  "barrelsA08",
  "cityA08",
  "cityA08U",
  "co2A",
  "co2TailpipeAGpm",
  "combA08",
  "combA08U",
  "fuelCostA08",
  "fuelType2",
  "highwayA08",
  "highwayA08U",
  "rangeA",
  "rangeCityA",
  "rangeHwyA",
  "UCityA",
  "UHighwayA",
  "charge120",
  "charge240",
  "sCharger",
  "tCharger",
  "c240Dscr",
  "charge240b",
  "c240bDscr",
  "phevHwy",
  "phevCity",
  "phevComb",
  "modifiedOn",
  "cityCD",
  "combinedCD",
  "highwayCD",
  "phevBlended",
  "cityE",
  "combE",
  "combinedUF",
  "evMotor",
  "highwayE",
  "highwayUF",
  "cityUF",
  "rangeCity",
  "rangeHwy",
  "range",
  "hlv",
  "hpv",
  "pv2",
  "pv4",
  "lv2",
  "lv4",
  "comb08U",
  "city08U",
  "highway08U",
  "engId",
  "id",
  "co2TailpipeGpm"
)
cleaned_df <- df %>% select(-all_of(unwantedlist))
```
```{r}
dim(cleaned_df)
```
```{r}
#considering only from year 2013
cleaned_df <- cleaned_df[cleaned_df$year > 2012,]

cleaned_df <- cleaned_df %>%
  select(-year)

#removing ev vehicles
cleaned_df <- cleaned_df[!is.na(cleaned_df$cylinders) & cleaned_df$cylinders != "Electricity",]
```
```{r}
dim(cleaned_df)
```
```{r}
#remove columns with only 1 unique value
cleaned_df <- cleaned_df %>%
  select(where(~ length(unique(.[. != ""])) > 1))

dim(cleaned_df)
```
```{r}
#check the number of missing values for each column
missing_values <- colSums(is.na(cleaned_df))
missing_values
```
```{r}
#check the number of value 0 for each column
zero_count_per_column <- cleaned_df %>%
  summarise(across(everything(), ~sum(. == 0, na.rm = TRUE)))

zero_count_per_column
```
```{r}
# Identify categorical columns and their unique value counts
categorical_columns <- sapply(cleaned_df, function(col) is.character(col) || is.factor(col))
unique_values <- sapply(cleaned_df[, categorical_columns], function(col) length(unique(col)))

# Separate columns for one-hot and label encoding
one_hot_columns <- names(unique_values[unique_values <= 10]) # Threshold for one-hot encoding
label_columns <- names(unique_values[unique_values > 10])    # For large categorical variables

# Applying One-Hot Encoding
if (length(one_hot_columns) > 0) {
  # Automatically drop one level per categorical variable to avoid multicollinearity
  one_hot_encoded <- dummyVars(~ ., data = cleaned_df[one_hot_columns], fullRank = TRUE)
  one_hot_df <- data.frame(predict(one_hot_encoded, newdata = cleaned_df))
}

# Applying Label Encoding
for (column in label_columns) {
  # Convert categories to numeric (ensure they are not interpreted as ordinal values)
  cleaned_df[[column]] <- as.numeric(as.factor(cleaned_df[[column]]))
}

# Combining One-Hot Encoded Data and Remaining Data
# Drop original one-hot columns from the dataset to avoid redundancy
final_data <- bind_cols(cleaned_df %>% select(-all_of(one_hot_columns)), one_hot_df)

#View the first few rows of the encoded dataset
print(head(final_data))
```
```{r}
alias(lm(comb08 ~ ., data = final_data))
```
```{r}
library(car)
library(tidyverse)

vif_model <- lm(feScore ~ ., data = selected_data)

vif_values <- vif(vif_model)

vif_df <- as.data.frame(vif_values) %>%
  rownames_to_column(var = "Variable")
print(vif_df)
```
```{r}
lm_data <- selected_data %>%
  select(-barrels08)

vif_model <- lm(feScore ~ ., data = lm_data)

vif_values <- vif(vif_model)

vif_df <- as.data.frame(vif_values) %>%
  rownames_to_column(var = "Variable")
print(vif_df)
```
```{r}
lm_data <- lm_data %>%
  select(-youSaveSpend)

vif_model <- lm(feScore ~ ., data = lm_data)

vif_values <- vif(vif_model)

vif_df <- as.data.frame(vif_values) %>%
  rownames_to_column(var = "Variable")
print(vif_df)
```
```{r}
lm_data <- lm_data %>%
  select(-UHighway)

vif_model <- lm(feScore ~ ., data = lm_data)

vif_values <- vif(vif_model)

vif_df <- as.data.frame(vif_values) %>%
  rownames_to_column(var = "Variable")
print(vif_df)
```
```{r}
lm_data <- lm_data %>%
  select(-co2TailpipeGpm)

vif_model <- lm(feScore ~ ., data = lm_data)

vif_values <- vif(vif_model)

vif_df <- as.data.frame(vif_values) %>%
  rownames_to_column(var = "Variable")
print(vif_df)
```
```{r}
#Building a regression tree
tree_model <- rpart(feScore ~ ., data = lm_data, method = "anova")

#Plotting the built tree
plot(tree_model, uniform=TRUE, main="Classification Tree")
text(tree_model, use.n=TRUE, all=TRUE, cex=.8)
```
```{r}
# Load necessary libraries
library(rpart)
library(caret)

# Split the data into training and testing sets
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(lm_data$feScore, p = 0.7, list = FALSE)
trainData <- lm_data[trainIndex, ]
testData <- lm_data[-trainIndex, ]

# Train a regression tree
tree_fit <- rpart(feScore ~ ., data = trainData, method = "anova")

# Visualize the tree
plot(tree_fit, uniform = TRUE, main = "Regression Tree")
text(tree_fit, use.n = TRUE, all = TRUE, cex = 0.8)

# Enhanced visualization (optional)
library(rpart.plot)
rpart.plot(tree_fit, main = "Regression Tree", type = 3, extra = 101, cex = 0.8)

# Make predictions on the test set
predictions <- predict(tree_fit, newdata = testData)

# Evaluate model performance
mse <- mean((testData$feScore - predictions)^2)
rmse <- sqrt(mse)
r_squared <- 1 - sum((testData$feScore - predictions)^2) / sum((testData$feScore - mean(testData$feScore))^2)

# Print metrics
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("R-Squared:", r_squared, "\n")
```
```{r}
# Calculate the 10th percentile of feScore
lower_bound <- quantile(testData$feScore, 0.1)

# Subset data for the lowest 10%
lowest_10 <- testData[testData$feScore <= lower_bound, ]
```
```{r}
# Generate predictions for the lowest 10%
lowest_10_predictions <- predict(tree_fit, newdata = lowest_10)

# Calculate RMSE and R² for the lowest 10%
mse_lowest_10 <- mean((lowest_10$feScore - lowest_10_predictions)^2)
rmse_lowest_10 <- sqrt(mse_lowest_10)
r_squared_lowest_10 <- 1 - sum((lowest_10$feScore - lowest_10_predictions)^2) /
                             sum((lowest_10$feScore - mean(lowest_10$feScore))^2)

cat("RMSE for lowest 10%:", rmse_lowest_10, "\n")
cat("R² for lowest 10%:", r_squared_lowest_10, "\n")
```
```{r}
library(caret)
feature_importance <- varImp(tree_fit)
print(feature_importance)
```
```{r}
library(ggplot2)

# Convert importance scores to a data frame, keeping correct row names
importance_df <- as.data.frame(feature_importance$Overall)
importance_df$Feature <- rownames(feature_importance)  # Preserve proper feature names
colnames(importance_df) <- c("Importance", "Feature")  # Rename columns for clarity

top_10_features <- importance_df[order(-importance_df$Importance), ][1:10, ]

# Create a bar chart
ggplot(top_10_features, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Feature Importance", x = "Feature", y = "Importance Score") +
  theme_minimal()

# Check the structure of the data frame
print(importance_df)
```